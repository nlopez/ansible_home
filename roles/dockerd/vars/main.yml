---
kubernetes_packages:
  - { name: kubelet, version: 1.9.3-00 }
  - { name: kubeadm, version: 1.9.3-00 }
  - { name: kubectl, version: 1.9.3-00 }
  - { name: kubernetes-cni, version: 0.6.0-00 }
docker_daemon_port: 2376
#nfs_mounts:
zfs_mounts:
  - { name: rpool/dockerfs, mountpoint: /dockerfs }
nfs_utils: nfs-utils
docker_sysctls:
  net.ipv4.ip_forward: 1
  net.bridge.bridge-nf-call-ip6tables: 1
  net.bridge.bridge-nf-call-iptables: 1
  net.netfilter.nf_conntrack_max: 1000000
  net.core.somaxconn: 1024
  vm.panic_on_oom: 1
  kernel.panic: 10
  # http://www.nateware.com/linux-network-tuning-for-2013.html
  # Increase Linux autotuning TCP buffer limits
  # Set max to 16MB for 1GE and 32M (33554432) or 54M (56623104) for 10GE
  # Don't set tcp_mem itself! Let the kernel scale it based on RAM.
  net.core.rmem_max: 16777216
  net.core.wmem_max: 16777216
  net.core.rmem_default: 16777216
  net.core.wmem_default: 16777216
  net.core.optmem_max: 40960
  # cloudflare uses this for balancing latency and throughput
  # https://blog.cloudflare.com/the-story-of-one-latency-spike/
  net.ipv4.tcp_rmem: 4096 1048576 2097152
  net.ipv4.tcp_wmem: 4096 65536 16777216
  # Also increase the max packet backlog
  net.core.netdev_max_backlog: 100000
  net.core.netdev_budget: 50000
  # Make room for more TIME_WAIT sockets due to more clients,
  # and allow them to be reused if we run out of sockets
  net.ipv4.tcp_max_syn_backlog: 30000
  net.ipv4.tcp_max_tw_buckets: 2000000
  net.ipv4.tcp_tw_reuse: 1
  net.ipv4.tcp_fin_timeout: 10
  # Disable TCP slow start on idle connections
  net.ipv4.tcp_slow_start_after_idle: 0
  # If your servers talk UDP, also up these limits
  net.ipv4.udp_rmem_min: 8192
  net.ipv4.udp_wmem_min: 8192
